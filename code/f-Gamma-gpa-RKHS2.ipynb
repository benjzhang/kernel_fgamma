{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92341f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--plot_intermediate_result'], dest='plot_intermediate_result', nargs=None, const=None, default=False, type=<class 'bool'>, choices=None, required=False, help='True -> save intermediate plots', metavar=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set parameters\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Data\n",
    "parser.add_argument(\n",
    "    '-N_Q', '--N_samples_Q', type=int, default=50, help='total number of target samples',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-N_P', '--N_samples_P', type=int, default=50, help='total number of prior samples',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-N_dim', type=int, help='dimension of input data',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-N_latent_dim', type=int, help='dimension of latent space',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-N_project_dim', type=int, help='dimension of PCA projected space on input',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-sample_latent', type=bool, default = False, help='True: sample in the latent space, False: sample in the physical space',\n",
    ")\n",
    "# Dataset property\n",
    "parser.add_argument(\n",
    "    '--dataset', type=str, default='Learning_gaussian', choices=['Lorenz63', 'Learning_gaussian', 'Mixture_of_gaussians', 'Mixture_of_gaussians2','Mixture_of_gaussians3','Mixture_of_gaussians4', 'Stretched_exponential', 'Learning_student_t', 'Mixture_of_student_t', 'Mixture_of_student_t_submnfld', 'Mixture_of_gaussians_submnfld','MNIST', 'CIFAR10', 'MNIST_switch', 'CIFAR10_switch', 'MNIST_ae', 'MNIST_ae_switch','CIFAR10_ae',  'Mixture_of_gaussians_submnfld_ae','BreastCancer', '1D_pts', '2D_pts','1D_dirac2gaussian', '1D_dirac2uniform',]\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-y0', type=float, nargs=\"+\", default=[1.0,2.0, 2.0]\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-beta', type=float, help='gibbs distribution of -|x|^\\beta',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-sigma_P', type=float, default=0.5, help='std of initial gaussian distribution',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-sigma_Q', type=float, default=1.0, help='std of target gaussian distribution',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-nu', type=float, help='df of target student-t distribution',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-interval_length', type=float, help='interval length of the uniform distribution',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-label', type=int, nargs=\"+\", help='class label of image data',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-pts_P', type=float, nargs=\"+\", default=[10.0,]\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-pts_Q', type=float, nargs=\"+\", default=[0.0,]\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-pts_P_2', type=float, nargs=\"+\", default=[0.0,]\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-pts_Q_2', type=float, nargs=\"+\", default=[0.0,]\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--random_seed', type=int, default=0, help='random seed for data generator',\n",
    ")\n",
    "\n",
    "\n",
    "# (f, Gamma)-divergence\n",
    "parser.add_argument(\n",
    "    '--f', type=str, default='KL', choices=['KL', 'alpha', 'reverse_KL', 'JS'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-alpha', type=float, help='parameter value for alpha divergence',\n",
    ")    \n",
    "parser.add_argument( \n",
    "    '--formulation', type=str, default='DV', choices=['LT', 'DV'], help='LT or DV in case of f=KL, otherwise, keep LT',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--Gamma', type=str, default='Lipshitz', choices=['Lipshitz'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-L', type=float, default=1.0, help='Lipshitz constant: default=inf w/o constraint',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--reverse', type=bool, default=False, help='True -> D(Q|P), False -> D(P|Q)',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--constraint', type=str, default='hard', choices=['hard', 'soft'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-lamda', type=float, default=0.05, help='coefficient on soft constraint',\n",
    ")\n",
    "\n",
    "\n",
    "# RKHS definition <phi>\n",
    "parser.add_argument(\n",
    "    '--kernel', type=str, default='gaussian', choices=['gaussian'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-bandwidth', type=float, default = 20.0, help='gaussian kernel bandwidth',\n",
    ")\n",
    "\n",
    "\n",
    "# training parameters\n",
    "parser.add_argument(\n",
    "    '-ep', '--epochs', type=int, default=500, help='# updates for P',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-ep_phi', '--epochs_phi', type=int, default=10, help='# updates to find phi*',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--optimizer', type=str, choices=['Newton', 'BFGS','Gradient_Ascent'], default='Gradient_Ascent', help='optimization method for phi',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--ode_solver', type=str, choices=['forward_euler', 'AB2', 'AB3', 'AB4', 'AB5', 'ABM1', 'Heun', 'ABM2', 'ABM3', 'ABM4', 'ABM5', 'RK4', 'ode45' ], default='forward_euler', help='ode solver for particle ode',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-mobility', type=str, help='problem dependent mobility function\\nRecommendation: MNIST - bounded',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '-lr_P_decay', type=str, choices=['rational', 'step',], help='delta t decay',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--lr_P', type=float, default=0.5, help='lr for P',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--lr_phi', type=float, default=0.005, help='lr for phi',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--exp_no', type=str, default='rkhs2', help='short experiment name under the same data',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--mb_size_P', type=int, default=50, help='mini batch size for the moving distribution P',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--mb_size_Q', type=int, default=50, help='mini batch size for the target distribution Q',\n",
    ")\n",
    "\n",
    "\n",
    "# save/display \n",
    "parser.add_argument(\n",
    "    '--save_iter', type=int, default=10, help='save results per each save_iter',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--plot_result', type=bool, default=True, help='True -> show plots',\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--plot_intermediate_result', type=bool, default=False, help='True -> save intermediate plots',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a0bf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2be9c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameters --------------------------------------\n",
    "p, unknown = parser.parse_known_args()\n",
    "param = vars(p)\n",
    "\n",
    "if p.alpha:    \n",
    "    par = [p.alpha]\n",
    "    p.exptype = '%s=%05.2f-%s' % (p.f, p.alpha, p.Gamma)\n",
    "else: \n",
    "    par = []\n",
    "    p.exptype = '%s-%s' % (p.f, p.Gamma)\n",
    "if p.L == None:\n",
    "    p.expname = '%s_%s' % (p.exptype, 'inf')\n",
    "else:\n",
    "    p.expname = '%s_%.4f' % (p.exptype, p.L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e0b7dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation ----------------------------------------\n",
    "from util.generate_data import generate_data\n",
    "# X_ ~ Q, Y_ ~ P_0\n",
    "p, X_, Y_, X_label, Y_label = generate_data(p)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96d319b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator learning  -----------------------------------------\n",
    "# kernel for RKHS\n",
    "def gaussian_kernel(x, y, bandwidth):\n",
    "    return np.exp(-np.sum(((x-y)/bandwidth)**2)/2)\n",
    "    \n",
    "def kernel(X_, Y_, bandwidth=1):\n",
    "    res = np.zeros((X_.shape[0],Y_.shape[0]))\n",
    "    for i, x in enumerate(X_):\n",
    "        for j, y in enumerate(Y_):\n",
    "            res[i,j] = gaussian_kernel(x, y, bandwidth=bandwidth)\n",
    "    return res\n",
    "    \n",
    "k = partial(kernel, bandwidth=p.bandwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c38d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Discriminator) Loss ----------------------------------------------\n",
    "if p.f == \"KL\":\n",
    "    f = lambda x: x * np.log(x)\n",
    "    f_star = lambda x: np.exp(x - 1)\n",
    "    f_star_prime = lambda x: np.exp(x - 1)\n",
    "    f_star_2prime = lambda x: np.exp(x - 1)\n",
    "elif p.f == 'alpha':\n",
    "    f = lambda x: (x**p.alpha - 1)/(p.alpha*(p.alpha-1))\n",
    "    f_star = lambda x: 1/p.alpha*(1/(p.alpha-1) + ((p.alpha-1)*np.maximum(x,0))**(p.alpha/(p.alpha-1)))\n",
    "    f_star_prime = lambda x: x**(1/(p.alpha-1)) if x>0 else 0\n",
    "    f_star_2prime = lambda x: 1/(p.alpha-1)*x**(1/(p.alpha-1)-1)\n",
    "\n",
    "\n",
    "def loss(X_, Y_, Z_, alpha, lamda):\n",
    "# loss = \\sum_i alpha_i f'(n*alpha_i) - \\sum_i f^*(f'(n*alpha_i))/n +\n",
    "#        1/2* (\\sum_i \\sum_j k(Y_i, Y_j)/n^2 - 2/n* alpha_i * k(X_i, Y_j) + alpha_i*alpha_j * K(X_i, X_j)\n",
    "    N_samples_P = Y_.shape[0]\n",
    "    N_samples_Q = X_.shape[0]\n",
    "    N_Z = 1.0#Z_.shape[0]\n",
    "    \n",
    "    regularizer = -p.lamda/2*max((alpha @ k(Z_, Z_) @ alpha/(N_Z*p.L)**2 -1,0))\n",
    "    return np.sum(k(Y_, Z_) @ alpha)/(N_Z*N_samples_P) - np.sum(f_star(k(X_, Z_) @ alpha/N_Z))/N_samples_Q +regularizer\n",
    "    \n",
    "def grad_loss(X_, Y_, Z_, alpha):\n",
    "    N_samples_P = Y_.shape[0]\n",
    "    N_samples_Q = X_.shape[0]\n",
    "    N_Z = 1.0#Z_.shape[0]\n",
    "    \n",
    "    grad_regularizer = 0\n",
    "    if alpha @ k(Z_, Z_) @ alpha > (N_Z*p.L)**2:\n",
    "        grad_regularizer = p.lamda/(N_Z*p.L)**2 * k(Z_, Z_) @ alpha\n",
    "    #print(np.round(np.sum(k(Y_, Z_), axis=0)/(N_Z*N_samples_P),4), np.round(k(Z_, X_) @ f_star_prime(k(X_, Z_) @ alpha/N_Z)/(N_Z*N_samples_Q),4), np.round(grad_regularizer,4))\n",
    "    return np.sum(k(Y_, Z_), axis=0)/(N_Z*N_samples_P) - k(Z_, X_) @ f_star_prime(k(X_, Z_) @ alpha/N_Z)/(N_Z*N_samples_Q) - grad_regularizer\n",
    "    \n",
    "def hess_loss(X_, Y_, Z_, alpha):\n",
    "    N_samples_P = Y_.shape[0]\n",
    "    N_samples_Q = X_.shape[0]\n",
    "    N_Z = 1.0#Z_.shape[0]\n",
    "    \n",
    "    hess_regularizer = 0\n",
    "    if alpha @ k(Z_, Z_) @ alpha > (N_Z*p.L)**2:\n",
    "        hess_regularizer =  p.lamda/(N_Z*p.L)**2 *k(Z_, Z_)\n",
    "    \n",
    "    return k(Z_, X_) @ np.diag(f_star_2prime(k(X_, Z_)@alpha)) @ k(X_, Z_)/(N_samples_Q * N_Z**2) - hess_regularizer\n",
    " \n",
    "def Gradient_Aescent(alpha, lr_phi, grad):\n",
    "    return alpha + lr_phi*grad\n",
    "    \n",
    "def Newton(alpha, lr_phi, grad, hess):\n",
    "    return alpha - lr_phi*np.linalg.inv(hess) @ grad\n",
    "    \n",
    "def BFGS(alpha, lr_phi, grad, B_inv, X_, Y_, Z_):\n",
    "    #print(\"Eigenvalues of inverse hessian:\", np.linalg.eigvals(B_inv))\n",
    "    p_k = - B_inv @ grad\n",
    "    s_k = lr_phi * p_k\n",
    "    alpha = alpha + s_k\n",
    "    y_k = grad_loss(X_, Y_, Z_, alpha) - grad\n",
    "    B_inv += (np.dot(s_k, y_k) + np.sum((B_inv * np.outer(y_k, y_k)).flatten()))/np.dot(s_k, y_k)**2 * np.outer(s_k, s_k) - (B_inv @ np.outer(y_k, s_k.T) + np.outer(s_k, y_k) @ B_inv)/np.dot(s_k, y_k)\n",
    "    return alpha, B_inv\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da9e2d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transporting particles --------------------------------------------\n",
    "# gradient of first variation\n",
    "def grad_gaussian_kernel(x, y, bandwidth):\n",
    "# gradient with respect to first argument (x) of gaussian kernel k(x,y)\n",
    "    return - (x-y)/bandwidth**2 * gaussian_kernel(x,y,bandwidth)\n",
    "    \n",
    "def grad_kernel(X_, Y_, bandwidth):\n",
    "    res = np.zeros((X_.shape[0], Y_.shape[1], Y_.shape[0]))\n",
    "    for i, x in enumerate(X_):\n",
    "        for j, y in enumerate(Y_):\n",
    "            res[i,:,j] = grad_gaussian_kernel(x, y, bandwidth)\n",
    "    return res\n",
    "    \n",
    "grad_k = partial(grad_kernel, bandwidth=p.bandwidth)\n",
    "    \n",
    "def grad_loss_first_variation(y, Z_, alpha):\n",
    "    N_Z = 1.0#Z_.shape[0]\n",
    "    return grad_k(y, Z_) @ alpha/N_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ca36a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ODE solver setting\n",
    "#from util.transport_particles import calc_vectorfield, solve_ode\n",
    "dPs = []\n",
    "if p.ode_solver in ['forward_euler', 'AB2', 'AB3', 'AB4', 'AB5']:\n",
    "    aux_params = []\n",
    "else:\n",
    "    aux_params = {'parameters': parameters, 'phi': phi, 'Q': Q, 'lr_phi': lr_phi,'epochs_nn': p.epochs_nn, 'loss_par': loss_par, 'NN_par': NN_par, 'data_par': data_par, 'optimizer': p.optimizer}\n",
    "\n",
    "# Applying mobility to particles\n",
    "#if p.mobility == 'bounded':\n",
    "    #from util.construct_NN import bounded_relu  # mobility that bounding particles (For image data)\n",
    "        \n",
    "# Train setting\n",
    "lr_P_init = p.lr_P # Assume that deltat = deltat(t)\n",
    "if p.ode_solver == \"DOPRI5\": # deltat = deltat(x,t)\n",
    "    lr_P_init = [p.lr_P]*p.N_samples_P\n",
    "    # Low dimensional example=> rank 2, Image example=> rank 4\n",
    "    for i in range(1, Y_.ndim):\n",
    "        lr_P_init = np.expand_dims(lr_P_init, axis=i)\n",
    "lr_P = lr_P_init\n",
    "lr_Ps = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b838782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save & plot settings -----------------------------------------------\n",
    "# Metrics to calculate\n",
    "from util.evaluate_metric import calc_ke, calc_grad_phi\n",
    "if \"MNIST\" in p.dataset or \"CIFAR10\" in p.dataset:\n",
    "    from util.evaluate_metric import calc_fid\n",
    "trajectories = []\n",
    "vectorfields = []\n",
    "divergences = []\n",
    "KE_Ps = []\n",
    "FIDs = []\n",
    "\n",
    "# saving/plotting parameters\n",
    "if p.save_iter >= p.epochs:\n",
    "    p.save_iter = 1\n",
    "\n",
    "if p.plot_result == True:\n",
    "    from plot_result import plot_result\n",
    "\n",
    "p.expname = p.expname+'_%04d_%04d_%02d_%s' % (p.N_samples_Q, p.N_samples_P, p.random_seed, p.exp_no)\n",
    "filename = p.dataset+'/%s.pickle' % (p.expname)\n",
    "\n",
    "if p.plot_intermediate_result == True:\n",
    "    if 'gaussian' in p.dataset and 'Extension' not in p.dataset:\n",
    "         r_param = p.sigma_Q\n",
    "    elif 'student_t' in p.dataset:\n",
    "        r_param = p.nu\n",
    "    elif p.dataset == 'Extension_of_gaussian':\n",
    "        r_param = p.a\n",
    "    else:\n",
    "        r_param = None\n",
    "    \n",
    "# additional plots for simple low dimensional dynamics\n",
    "if p.N_dim == 1:\n",
    "    xx = np.linspace(-10, 10, 300)\n",
    "    phis = []\n",
    "elif p.N_dim == 2:#'2D' in p.dataset:\n",
    "    xx = np.linspace(-10, 10, 40)\n",
    "    yy = np.linspace(-10, 10, 40)\n",
    "    XX, YY = np.meshgrid(xx, yy)\n",
    "    xx = np.concatenate((np.reshape(XX, (-1,1)), np.reshape(YY, (-1,1))), axis=1)\n",
    "    phis = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1807b764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter     50: loss = 1.8885332713, kinetic energy of P = 0.0162056265, average learning rate for P = 0.500000\n",
      "grad 0.17935612727676792\n",
      "iter    100: loss = 0.8672834429, kinetic energy of P = 0.0140083605, average learning rate for P = 0.500000\n",
      "grad 0.16688220495043982\n",
      "iter    150: loss = 0.2627192089, kinetic energy of P = 0.0061928373, average learning rate for P = 0.500000\n",
      "grad 0.11100995978022561\n",
      "iter    200: loss = 0.0546722640, kinetic energy of P = 0.0010836378, average learning rate for P = 0.500000\n",
      "grad 0.04626666714723179\n",
      "iter    250: loss = 0.0103962677, kinetic energy of P = 0.0003395506, average learning rate for P = 0.500000\n",
      "grad 0.025616038350699758\n",
      "iter    300: loss = -0.0035408534, kinetic energy of P = 0.0001465327, average learning rate for P = 0.500000\n",
      "grad 0.016570198249062126\n",
      "iter    350: loss = -0.0066489437, kinetic energy of P = 0.0000935281, average learning rate for P = 0.500000\n",
      "grad 0.013138763213038305\n",
      "iter    400: loss = -0.0100627886, kinetic energy of P = 0.0001309926, average learning rate for P = 0.500000\n",
      "grad 0.01558184501834447\n",
      "iter    450: loss = -0.0108728293, kinetic energy of P = 0.0001063155, average learning rate for P = 0.500000\n",
      "grad 0.013960449316696448\n"
     ]
    }
   ],
   "source": [
    "# Train ---------------------------------------------------------------\n",
    "import time \n",
    "t0 = time.time()\n",
    "\n",
    "alpha = np.zeros(int(p.N_samples_Q/2)+int(p.N_samples_P/2))\n",
    "for it in range(1, p.epochs+1): # Loop for updating particles P\n",
    "    X_idx_for_Z_ = np.random.choice(p.N_samples_Q, int(p.N_samples_Q/2), replace=False)\n",
    "    Y_idx_for_Z_ = np.random.choice(p.N_samples_P, int(p.N_samples_P/2), replace=False)\n",
    "    Z_ = np.vstack((X_[X_idx_for_Z_], Y_[Y_idx_for_Z_]))\n",
    "    #print(f_star_2prime(k(X_, Z_)@alpha), alpha)\n",
    "    for in_it in range(p.epochs_phi):\n",
    "        grad = grad_loss(X_, Y_, Z_, alpha)\n",
    "        if p.optimizer == 'Gradient_Ascent':\n",
    "            alpha = Gradient_Aescent(alpha, p.lr_phi, grad)\n",
    "        elif p.optimizer == 'Newton':\n",
    "            hess = hess_loss(X_, Y_, Z_, alpha)\n",
    "            alpha = Newton(alpha, p.lr_phi, grad, hess)\n",
    "        elif p.optimizer == 'BFGS':\n",
    "            if in_it == 0:\n",
    "                #hess = hess_loss(X_, Y_, Z_, alpha)\n",
    "                hess = np.identity(len(alpha))\n",
    "                B_inv = np.linalg.inv(hess)\n",
    "                alpha, B_inv = BFGS(alpha, p.lr_phi, grad, B_inv, X_, Y_, Z_)\n",
    "    current_loss = loss(X_, Y_, Z_, alpha, p.lamda)\n",
    "            \n",
    "    dP = grad_loss_first_variation(Y_, Z_, alpha)\n",
    "    dPs.append(dP)\n",
    "    \n",
    "    Y_ = Y_ - lr_P * dPs[-1]\n",
    "    '''\n",
    "    if p.ode_solver == \"DOPRI5\": # deltat adust\n",
    "        P, dPs, dP, lr_P = solve_ode(P, lr_P, dPs, p.ode_solver, aux_params) # update P\n",
    "    else:\n",
    "        P, dPs, dP = solve_ode(P, lr_P, dPs, p.ode_solver, aux_params) # update P\n",
    "    '''\n",
    "\n",
    "    if p.mobility == 'bounded':\n",
    "        Y_ = bounded_relu(Y_)\n",
    "     \n",
    "    lr_Ps.append(lr_P)\n",
    "    # adjust learning rates\n",
    "    #if it>=100:\n",
    "    #    lr_P = decay_learning_rate(lr_P, p.lr_P_decay, {'epochs': p.epochs-100, 'epoch': it-100, 'KE_P': KE_P})\n",
    "    \n",
    "    # save results\n",
    "    divergences.append(current_loss)\n",
    "    KE_P = calc_ke(dP, p.N_samples_P)\n",
    "    KE_Ps.append(KE_P)\n",
    "    grad_phi = calc_grad_phi(dP)\n",
    "    #print(\"grad\", grad_phi)\n",
    "    \n",
    "    if p.epochs<=100 or it%p.save_iter == 0:\n",
    "        if p.dataset in ['BreastCancer',]:\n",
    "            trajectories.append(Y_*10)\n",
    "        else:\n",
    "            trajectories.append(Y_)\n",
    "        if np.prod(p.N_dim) < 500:\n",
    "            vectorfields.append(dP)\n",
    "        elif np.prod(p.N_dim) >= 784:  # image data\n",
    "            FIDs.append( calc_fid(pred=Y_, real=X_) )\n",
    "    \n",
    "    # display intermediate results\n",
    "    if it % (p.epochs/10) == 0:\n",
    "    #if it in [5, 50, 500, 1000, 2000, 3000, 4000, 5000]:\n",
    "        display_msg = 'iter %6d: loss = %.10f, kinetic energy of P = %.10f, average learning rate for P = %.6f' % (it, current_loss, KE_P, np.mean(lr_P))\n",
    "        if np.prod(p.N_dim) >= 784:\n",
    "            display_msg = display_msg + ', FID = %.3f' % FIDs[-1]\n",
    "        print(display_msg)\n",
    "        print(\"grad\", grad_phi)\n",
    "        \n",
    "        if p.plot_intermediate_result == True:\n",
    "            data = {'trajectories': trajectories, 'divergences': divergences, 'KE_Ps': KE_Ps, 'FIDs':FIDs, 'X_':X_, 'Y_':Y_, 'X_label':X_label, 'Y_label':Y_label, 'dt': lr_Ps, 'dataset': p.dataset, 'r_param': r_param, 'vectorfields': vectorfields, 'save_iter':p.save_iter}\n",
    "            if p.N_dim ==2:\n",
    "                data.update({'phi': phi, 'W':W, 'b':b, 'NN_par':NN_par})\n",
    "            plot_result(filename, intermediate=True, epochs = it, iter_nos = None, data = data, show=False)\n",
    "        \n",
    "        '''\n",
    "        if np.prod(p.N_dim) <= 2:\n",
    "            zz = phi(xx,None, W,b,NN_par).numpy()\n",
    "            zz = np.reshape(zz, -1)\n",
    "            phis.append(zz)\n",
    "        '''\n",
    "\n",
    "total_time = time.time() - t0\n",
    "print(f'total time {total_time:.3f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42daf473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result ------------------------------------------------------\n",
    "import pickle\n",
    "if not os.path.exists(p.dataset):\n",
    "    os.makedirs(p.dataset)\n",
    "\n",
    "if '1D' in p.dataset:\n",
    "    X_ = np.concatenate((X_, np.zeros(shape=X_.shape)), axis=1)\n",
    "    Y_ = np.concatenate((Y_, np.zeros(shape=Y_.shape)), axis=1)\n",
    "    \n",
    "    trajectories = [np.concatenate((x, np.zeros(shape=x.shape)), axis=1) for x in trajectories]\n",
    "    vectorfields = [np.concatenate((x, np.zeros(shape=x.shape)), axis=1) for x in vectorfields]\n",
    "        \n",
    "param.update({'X_': X_, 'Y_': Y_, 'lr_Ps':lr_Ps,})\n",
    "result = {'trajectories': trajectories, 'vectorfields': vectorfields, 'divergences': divergences, 'KE_Ps': KE_Ps, 'FIDs': FIDs,}\n",
    "\n",
    "if p.dataset in ['BreastCancer',]:\n",
    "    np.savetxt(\"gene_expression_example/GPL570/\"+p.dataset+'/output_norm_dataset_dim_%d.csv' % p.N_dim, trajectories[-1], delimiter=\",\")\n",
    "        \n",
    "# Save trained data\n",
    "with open(filename,\"wb\") as fw:\n",
    "    pickle.dump([param, result] , fw)\n",
    "print(\"Results saved at:\", filename)\n",
    "\n",
    "# Plot final result\n",
    "if p.plot_result == True:\n",
    "    plot_result(filename, intermediate=False, show=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
